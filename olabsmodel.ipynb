{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "947HYA1aMPk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e27c08-a0a8-4fde-fa8f-9899eb41a3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-box in /usr/local/lib/python3.11/dist-packages (7.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faker numpy pandas python-box scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from box import Box\n",
        "import json\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = Box({\n",
        "    \"num_students\": 2000,\n",
        "    \"schools\": [\"KV_\"+str(i) for i in range(1, 21)],\n",
        "    \"classes\": [\"11A\", \"11B\", \"12A\", \"12B\"],\n",
        "    \"concepts\": {\n",
        "        \"core\": [\"molarity\", \"stoichiometry\", \"acid_base\", \"indicators\"],\n",
        "        \"secondary\": [\"dilution\", \"ph_scale\", \"neutralization\"]\n",
        "    },\n",
        "    \"error_types\": {\n",
        "        \"procedural\": [\"wrong_indicator\", \"burette_reading\", \"endpoint_missed\"],\n",
        "        \"conceptual\": [\"molarity_calculation\", \"ph_interpretation\", \"stoichiometry_error\"]\n",
        "    }\n",
        "})\n",
        "\n",
        "experiment_config = {\n",
        "    \"titration\": {\n",
        "        \"procedural_errors\": [\"wrong_indicator\", \"burette_handling\"],\n",
        "        \"conceptual_errors\": [\"molarity_calc\", \"equivalence_point\"],\n",
        "        \"related_concepts\": [\"acid_base\", \"indicators\", \"stoichiometry\"]\n",
        "    },\n",
        "    \"calorimetry\": {\n",
        "        \"procedural_errors\": [\"insulation_missing\", \"thermometer_placement\"],\n",
        "        \"conceptual_errors\": [\"heat_loss_calculation\", \"specific_heat\"],\n",
        "        \"related_concepts\": [\"thermodynamics\", \"energy_transfer\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "i9ck3_gnNFmr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake = Faker()\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_student():\n",
        "    student = {\n",
        "        \"student_id\": f\"STU_{fake.unique.bothify(text='????####')}\",\n",
        "        \"school\": np.random.choice(config.schools),\n",
        "        \"class\": np.random.choice(config.classes),\n",
        "        \"error_profile\": np.random.choice([\"novice\", \"theory_weak\", \"careless\"], p=[0.3, 0.4, 0.3]),\n",
        "        \"experiment_history\": []  # Always initialize as empty list\n",
        "    }\n",
        "\n",
        "    # Generate 2-3 random experiments per student\n",
        "    experiments = np.random.choice(\n",
        "        list(experiment_config.keys()),\n",
        "        size=np.random.randint(2, 4),  # Adjusted to 2-3 experiments\n",
        "        replace=True  # Allow sampling with replacement\n",
        "    )\n",
        "\n",
        "    for exp in experiments:\n",
        "        student[\"experiment_history\"].append(\n",
        "            generate_experiment_data(exp, student[\"error_profile\"])\n",
        "        )\n",
        "\n",
        "    return student\n",
        "\n",
        "def generate_experiment_data(exp_type, profile):\n",
        "    num_errors = np.random.poisson(lam=2) + 1\n",
        "    errors = []\n",
        "\n",
        "    for _ in range(num_errors):\n",
        "        # Use experiment-specific error config\n",
        "        if np.random.rand() < 0.6 if profile==\"novice\" else 0.3:\n",
        "            error_type = \"procedural\"\n",
        "            specific_error = np.random.choice(experiment_config[exp_type][\"procedural_errors\"])\n",
        "        else:\n",
        "            error_type = \"conceptual\"\n",
        "            specific_error = np.random.choice(experiment_config[exp_type][\"conceptual_errors\"])\n",
        "\n",
        "        errors.append({\n",
        "            \"error_type\": error_type,\n",
        "            \"specific_error\": specific_error,\n",
        "            \"concepts\": get_related_concepts(exp_type, specific_error),\n",
        "            \"severity\": np.random.randint(1,5)\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"experiment_type\": exp_type,\n",
        "        \"errors\": errors\n",
        "    }\n",
        "\n",
        "def get_related_concepts(exp_type, specific_error):\n",
        "    # Map specific errors to related concepts\n",
        "    if specific_error in experiment_config[exp_type][\"procedural_errors\"]:\n",
        "        return experiment_config[exp_type][\"related_concepts\"][:1]  # First concept\n",
        "    else:\n",
        "        return experiment_config[exp_type][\"related_concepts\"][1:]  # Remaining concepts"
      ],
      "metadata": {
        "id": "sm6bFdyRN0e0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "students = [generate_student() for _ in range(config.num_students)]\n",
        "\n",
        "# Validate student records\n",
        "for student in students:\n",
        "    if \"experiment_history\" not in student:\n",
        "        print(f\"Invalid student record: {student['student_id']}\")\n",
        "        student[\"experiment_history\"] = []  # Fix missing key\n",
        "\n",
        "# Add peer cluster patterns\n",
        "for school in config.schools:\n",
        "    # 1 common error per experiment type\n",
        "    common_errors = {\n",
        "        exp: np.random.choice(experiment_config[exp][\"procedural_errors\"])\n",
        "        for exp in experiment_config\n",
        "    }\n",
        "\n",
        "    # Apply to 30% students from this school\n",
        "    for student in students:\n",
        "        if student[\"school\"] == school and np.random.rand() < 0.3:\n",
        "            if \"experiment_history\" not in student:\n",
        "                student[\"experiment_history\"] = []  # Ensure key exists\n",
        "            for exp_history in student[\"experiment_history\"]:\n",
        "                if np.random.rand() < 0.4:  # 40% chance to add common error\n",
        "                    exp_type = exp_history[\"experiment_type\"]\n",
        "                    exp_history[\"errors\"].append({\n",
        "                        \"error_type\": \"procedural\",\n",
        "                        \"specific_error\": common_errors[exp_type],\n",
        "                        \"concepts\": experiment_config[exp_type][\"related_concepts\"][:1],\n",
        "                        \"severity\": 3\n",
        "                    })"
      ],
      "metadata": {
        "id": "Cp0emAhDOJAE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"labmate_dataset.json\", \"w\") as f:\n",
        "    json.dump({\"students\": students}, f, indent=2)\n",
        "\n",
        "!cp labmate_dataset.json \"/content/drive/MyDrive/\"  # Save to Google Drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIiBAZnHONRi",
        "outputId": "06a08c13-8f5f-4910-baf4-131f2654c317"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file '/content/drive/MyDrive/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Students: {len(students)}\")\n",
        "print(f\"Total Errors: {sum(len(exp['errors']) for s in students for exp in s['experiment_history'])}\")\n",
        "print(\"\\nSample Student:\")\n",
        "print(json.dumps(students[0], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUddUGLCOPxy",
        "outputId": "e5094b79-f11c-409e-b8c6-08b1e9d8645e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Students: 2000\n",
            "Total Errors: 15851\n",
            "\n",
            "Sample Student:\n",
            "{\n",
            "  \"student_id\": \"STU_VkKy6496\",\n",
            "  \"school\": \"KV_7\",\n",
            "  \"class\": \"12B\",\n",
            "  \"error_profile\": \"careless\",\n",
            "  \"experiment_history\": [\n",
            "    {\n",
            "      \"experiment_type\": \"calorimetry\",\n",
            "      \"errors\": [\n",
            "        {\n",
            "          \"error_type\": \"procedural\",\n",
            "          \"specific_error\": \"insulation_missing\",\n",
            "          \"concepts\": [\n",
            "            \"thermodynamics\"\n",
            "          ],\n",
            "          \"severity\": 4\n",
            "        },\n",
            "        {\n",
            "          \"error_type\": \"procedural\",\n",
            "          \"specific_error\": \"insulation_missing\",\n",
            "          \"concepts\": [\n",
            "            \"thermodynamics\"\n",
            "          ],\n",
            "          \"severity\": 4\n",
            "        },\n",
            "        {\n",
            "          \"error_type\": \"procedural\",\n",
            "          \"specific_error\": \"thermometer_placement\",\n",
            "          \"concepts\": [\n",
            "            \"thermodynamics\"\n",
            "          ],\n",
            "          \"severity\": 4\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"experiment_type\": \"titration\",\n",
            "      \"errors\": [\n",
            "        {\n",
            "          \"error_type\": \"procedural\",\n",
            "          \"specific_error\": \"burette_handling\",\n",
            "          \"concepts\": [\n",
            "            \"acid_base\"\n",
            "          ],\n",
            "          \"severity\": 4\n",
            "        },\n",
            "        {\n",
            "          \"error_type\": \"procedural\",\n",
            "          \"specific_error\": \"burette_handling\",\n",
            "          \"concepts\": [\n",
            "            \"acid_base\"\n",
            "          ],\n",
            "          \"severity\": 2\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "id": "EWuhzRkDpXkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b586b26-a416-497b-a199-90b828284bf5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def build_feature_matrix(students):\n",
        "    # Initialize feature matrix\n",
        "    feature_matrix = []\n",
        "\n",
        "    # Extract features for each student\n",
        "    for student in students:\n",
        "        features = {}\n",
        "\n",
        "        # School and class\n",
        "        features[\"school\"] = student[\"school\"]\n",
        "        features[\"class\"] = student[\"class\"]\n",
        "\n",
        "        # Error counts per experiment type\n",
        "        for exp in experiment_config:\n",
        "            for error_type in [\"procedural\", \"conceptual\"]:\n",
        "                key = f\"{exp}_{error_type}\"\n",
        "                features[key] = sum(\n",
        "                    1 for exp_history in student[\"experiment_history\"]\n",
        "                    for error in exp_history[\"errors\"]\n",
        "                    if exp_history[\"experiment_type\"] == exp and error[\"error_type\"] == error_type\n",
        "                )\n",
        "\n",
        "        # Concept mastery scores\n",
        "        for concept in config.concepts.core + config.concepts.secondary:\n",
        "            features[concept] = sum(\n",
        "                1 for exp_history in student[\"experiment_history\"]\n",
        "                for error in exp_history[\"errors\"]\n",
        "                if concept in error[\"concepts\"]\n",
        "            )\n",
        "\n",
        "        feature_matrix.append(features)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(feature_matrix)\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    encoder = OneHotEncoder()  # Remove `sparse=False`\n",
        "    encoded_features = encoder.fit_transform(df[[\"school\", \"class\"]]).toarray()  # Convert to dense array\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out([\"school\", \"class\"]))\n",
        "\n",
        "    # Combine with numerical features\n",
        "    final_df = pd.concat([df.drop(columns=[\"school\", \"class\"]), encoded_df], axis=1)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Build feature matrix\n",
        "feature_matrix = build_feature_matrix(students)\n",
        "print(feature_matrix.head())"
      ],
      "metadata": {
        "id": "7MAeIlbGpBdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e96e53-ad4e-4137-eda7-d3c286142ca9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   titration_procedural  titration_conceptual  calorimetry_procedural  \\\n",
            "0                     2                     0                       3   \n",
            "1                     0                     0                       6   \n",
            "2                     4                     0                       0   \n",
            "3                     7                     0                       3   \n",
            "4                     3                     4                       0   \n",
            "\n",
            "   calorimetry_conceptual  molarity  stoichiometry  acid_base  indicators  \\\n",
            "0                       0         0              0          2           0   \n",
            "1                       0         0              0          0           0   \n",
            "2                       0         0              0          4           0   \n",
            "3                       0         0              0          7           0   \n",
            "4                       0         0              4          3           4   \n",
            "\n",
            "   dilution  ph_scale  ...  school_KV_4  school_KV_5  school_KV_6  \\\n",
            "0         0         0  ...          0.0          0.0          0.0   \n",
            "1         0         0  ...          0.0          0.0          0.0   \n",
            "2         0         0  ...          0.0          0.0          0.0   \n",
            "3         0         0  ...          0.0          0.0          0.0   \n",
            "4         0         0  ...          0.0          0.0          0.0   \n",
            "\n",
            "   school_KV_7  school_KV_8  school_KV_9  class_11A  class_11B  class_12A  \\\n",
            "0          1.0          0.0          0.0        0.0        0.0        0.0   \n",
            "1          0.0          0.0          0.0        0.0        0.0        0.0   \n",
            "2          0.0          0.0          0.0        1.0        0.0        0.0   \n",
            "3          0.0          0.0          0.0        1.0        0.0        0.0   \n",
            "4          0.0          0.0          0.0        1.0        0.0        0.0   \n",
            "\n",
            "   class_12B  \n",
            "0        1.0  \n",
            "1        1.0  \n",
            "2        0.0  \n",
            "3        0.0  \n",
            "4        0.0  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm\n",
        "from lightfm import LightFM\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def prepare_cf_data(feature_matrix):\n",
        "    # Create user-item matrix (student x error_type)\n",
        "    error_types = list({f\"{exp}_{et}\" for exp in experiment_config for et in [\"procedural\", \"conceptual\"]})\n",
        "    user_item_matrix = np.zeros((len(feature_matrix), len(error_types)))\n",
        "\n",
        "    for i, row in feature_matrix.iterrows():\n",
        "        for j, error_type in enumerate(error_types):\n",
        "            user_item_matrix[i][j] = row[error_type]\n",
        "\n",
        "    return csr_matrix(user_item_matrix), error_types\n",
        "\n",
        "# Prepare data\n",
        "user_item_matrix, error_types = prepare_cf_data(feature_matrix)\n",
        "\n",
        "# Train model\n",
        "model = LightFM(loss='warp')  # Weighted Approximate-Rank Pairwise\n",
        "model.fit(user_item_matrix, epochs=20)\n",
        "\n",
        "# Predict procedural errors for titration\n",
        "def predict_errors(student_id, experiment_type=\"titration\"):\n",
        "    student_idx = feature_matrix.index[feature_matrix.index == student_id].tolist()[0]\n",
        "    scores = model.predict(student_idx, np.arange(len(error_types)))\n",
        "\n",
        "    # Filter for procedural errors in titration\n",
        "    target_errors = [et for et in error_types if et.startswith(f\"{experiment_type}_procedural\")]\n",
        "    target_indices = [error_types.index(et) for et in target_errors]\n",
        "\n",
        "    # Get top 3 predictions\n",
        "    top_indices = np.argsort(scores[target_indices])[-3:]\n",
        "    return [target_errors[i] for i in top_indices]\n",
        "\n",
        "# Example usage\n",
        "student_id = feature_matrix.index[0]  # First student\n",
        "print(f\"Predicted errors for student {student_id}: {predict_errors(student_id)}\")"
      ],
      "metadata": {
        "id": "ql18TtSJqkan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48687e3-83a2-4071-dd77-6a7aad4794d4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.11/dist-packages (1.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from lightfm) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (3.5.0)\n",
            "Predicted errors for student 0: ['titration_procedural']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "from lightfm import LightFM\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Previous code...\n",
        "\n",
        "# Split data into train/test sets\n",
        "train_matrix, test_matrix = train_test_split(user_item_matrix, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model on training data\n",
        "model = LightFM(loss='warp')\n",
        "model.fit(train_matrix, epochs=20)\n",
        "\n",
        "# Evaluate on test data\n",
        "def evaluate_model(model, test_matrix, error_types, experiment_type=\"titration\"):\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for student_idx in range(test_matrix.shape[0]):\n",
        "        # Get true errors\n",
        "        true_errors_indices = test_matrix[student_idx].nonzero()[1]\n",
        "        true_errors = [error_types[i] for i in true_errors_indices if error_types[i].startswith(f\"{experiment_type}_procedural\")]\n",
        "\n",
        "        # Predict errors\n",
        "        predicted_errors = predict_errors(student_idx, experiment_type)\n",
        "\n",
        "        # Check if there are any true errors\n",
        "        if not true_errors:\n",
        "            # If no true errors, skip precision/recall calculation for this student\n",
        "            continue\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        # We need to convert the lists of strings into an array that the precision and recall functions understand\n",
        "        true_array = np.array([1 if error in true_errors else 0 for error in error_types if error.startswith(f\"{experiment_type}_procedural\")])\n",
        "        predicted_array = np.array([1 if error in predicted_errors else 0 for error in error_types if error.startswith(f\"{experiment_type}_procedural\")])\n",
        "\n",
        "        precision = precision_score(true_array, predicted_array, average='micro', zero_division=0) # Added the zero_division parameter to resolve divide by zero warnings\n",
        "        recall = recall_score(true_array, predicted_array, average='micro', zero_division=0) # Added the zero_division parameter to resolve divide by zero warnings\n",
        "\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "\n",
        "    return np.mean(precision_scores), np.mean(recall_scores)\n",
        "\n",
        "# Evaluate\n",
        "precision, recall = evaluate_model(model, test_matrix, error_types)\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")"
      ],
      "metadata": {
        "id": "pSNY2dc2suav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a93a2c5-bc4a-4f6c-80f5-dabc585b6b3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00, Recall: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check error type distribution\n",
        "error_counts = feature_matrix[[f\"{exp}_{et}\" for exp in experiment_config for et in [\"procedural\", \"conceptual\"]]].sum()\n",
        "print(\"Error Type Distribution:\")\n",
        "print(error_counts)\n",
        "\n",
        "# Check concept mastery distribution\n",
        "concept_counts = feature_matrix[config.concepts.core + config.concepts.secondary].sum()\n",
        "print(\"\\nConcept Mastery Distribution:\")\n",
        "print(concept_counts)"
      ],
      "metadata": {
        "id": "AeVoLekHyjar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d93502-a22e-4ed1-8354-f6ea0a52c48e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Type Distribution:\n",
            "titration_procedural      6852\n",
            "titration_conceptual       857\n",
            "calorimetry_procedural    7246\n",
            "calorimetry_conceptual     896\n",
            "dtype: int64\n",
            "\n",
            "Concept Mastery Distribution:\n",
            "molarity             0\n",
            "stoichiometry      857\n",
            "acid_base         6852\n",
            "indicators         857\n",
            "dilution             0\n",
            "ph_scale             0\n",
            "neutralization       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Implementation\n",
        "#Data Preprocessing\n",
        "import json\n",
        "\n",
        "# Load the JSON file (update the filename if needed)\n",
        "with open(\"labmate_dataset.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "    students = data.get(\"students\", [])\n",
        "\n",
        "documents = []\n",
        "for student in students:\n",
        "    student_id = student.get(\"student_id\", \"UnknownID\")\n",
        "    for experiment in student.get(\"experiment_history\", []):\n",
        "        experiment_type = experiment.get(\"experiment_type\", \"UnknownExperiment\")\n",
        "        for error in experiment.get(\"errors\", []):\n",
        "            error_type = error.get(\"error_type\", \"UnknownError\")\n",
        "            specific_error = error.get(\"specific_error\", \"NoDetail\")\n",
        "            concepts = \", \".join(error.get(\"concepts\", []))\n",
        "            severity = error.get(\"severity\", \"N/A\")\n",
        "\n",
        "            doc = (f\"Student: {student_id}; Experiment: {experiment_type}; \"\n",
        "                   f\"Error: {error_type} - {specific_error}; Concepts: {concepts}; \"\n",
        "                   f\"Severity: {severity}\")\n",
        "            documents.append(doc)\n",
        "\n",
        "print(f\"Total Documents Created: {len(documents)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXmQimsRti1",
        "outputId": "09407572-36e3-4080-b26a-f71b3fcd2e81"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents Created: 15851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "bxB1axeVyJ6_",
        "outputId": "95a4551a-8fe5-4aed-ef28-17b2f908220d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "print(\"FAISS version:\", faiss.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2en_CTEyVsn",
        "outputId": "a1a293a3-45d0-4f28-d03c-685aa2a5b820"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS version: 1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9uWqq5PrtN",
        "outputId": "4d26d013-e66a-487f-c10d-effdecb3d8dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retreival\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "# Load data from labmate_data.json\n",
        "# Changed filename from 'labmate_dataset.json' to 'labmate_data.json'\n",
        "with open(\"labmate_dataset.json\", \"r\") as f:\n",
        "    labmate_data = json.load(f)\n",
        "    students = labmate_data.get(\"students\", [])\n",
        "\n",
        "# Create a list of documents by iterating over each student's experiment history\n",
        "documents = []\n",
        "for student in students:\n",
        "    student_id = student.get(\"student_id\", \"UnknownID\")\n",
        "    for experiment in student.get(\"experiment_history\", []):\n",
        "        experiment_type = experiment.get(\"experiment_type\", \"UnknownExperiment\")\n",
        "        for error in experiment.get(\"errors\", []):\n",
        "            error_type = error.get(\"error_type\", \"UnknownError\")\n",
        "            specific_error = error.get(\"specific_error\", \"NoDetail\")\n",
        "            concepts = \", \".join(error.get(\"concepts\", []))\n",
        "            severity = error.get(\"severity\", \"N/A\")\n",
        "            # Construct a document string capturing key details\n",
        "            doc = (f\"Student: {student_id}; Experiment: {experiment_type}; \"\n",
        "                   f\"Error Type: {error_type}; Specific Error: {specific_error}; \"\n",
        "                   f\"Concepts: {concepts}; Severity: {severity}\")\n",
        "            documents.append(doc)\n",
        "\n",
        "print(f\"Total Documents Created: {len(documents)}\")\n",
        "\n",
        "# Load a pretrained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "doc_embeddings = model.encode(documents)\n",
        "\n",
        "# Create a FAISS index with L2 distance\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(doc_embeddings, dtype=np.float32))\n",
        "\n",
        "# Save the FAISS index for later use (optional)\n",
        "faiss.write_index(index, \"faiss_index.bin\")\n",
        "print(\"FAISS index created and saved.\")\n",
        "\n",
        "# Function to retrieve context given a student query\n",
        "def retrieve_context(query, top_k=3):\n",
        "    query_vec = model.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vec, dtype=np.float32), top_k)\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Example usage\n",
        "query = \"Why did my titration fail?\"\n",
        "context_docs = retrieve_context(query)\n",
        "print(\"Retrieved Context:\", context_docs)"
      ],
      "metadata": {
        "id": "LEHQM77W6dIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4b52ed-1664-429a-a7f1-d5fad228a80d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Documents Created: 15851\n",
            "FAISS index created and saved.\n",
            "Retrieved Context: ['Student: STU_jdIY8865; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 4', 'Student: STU_TqDW5488; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 1', 'Student: STU_xwLa9843; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query, top_k=3):\n",
        "    query_vec = model.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vec, dtype=np.float32), top_k)\n",
        "    # Retrieve the corresponding documents\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Example query\n",
        "query = \"Why did my titration fail?\"\n",
        "context_docs = retrieve_context(query)\n",
        "print(\"Retrieved Context:\")\n",
        "for doc in context_docs:\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJiBbDo_V_Nn",
        "outputId": "16ac49f7-e616-40b6-80ad-aec8e99d017d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Context:\n",
            "Student: STU_jdIY8865; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 4\n",
            "Student: STU_TqDW5488; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 1\n",
            "Student: STU_xwLa9843; Experiment: titration; Error Type: procedural; Specific Error: wrong_indicator; Concepts: acid_base; Severity: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_szcv4UWa99",
        "outputId": "ae56170b-05bc-4997-8316-988d50876028"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRy5ZGdDWlgu",
        "outputId": "d28994a1-ce5b-4de3-eb25-db854e54f472"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVok9TfNW82e",
        "outputId": "d982474f-750b-40d2-e95b-a355ee120329"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.40)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS as LangchainFAISS\n"
      ],
      "metadata": {
        "id": "qkCRa0cDXFkX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a wrapper class for SentenceTransformer\n",
        "class SentenceTransformerWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts).tolist()\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode([text])[0].tolist()\n",
        "\n",
        "# Create an instance of the wrapper\n",
        "embedding_wrapper = SentenceTransformerWrapper(model)\n",
        "\n",
        "# Now create the vector store using the wrapper\n",
        "from langchain.vectorstores import FAISS as LangchainFAISS\n",
        "vector_store = LangchainFAISS.from_texts(documents, embedding=embedding_wrapper)\n",
        "\n",
        "# Initialize the LLM (e.g., GPT-4)\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Create a RetrievalQA chain\n",
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vector_store.as_retriever(), chain_type=\"stuff\")\n",
        "\n",
        "# Example query\n",
        "query = \"How can I improve my titration accuracy?\"\n",
        "final_response = qa_chain.run(query)\n",
        "print(\"Final Response:\", final_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "RUg-SBNDWrNd",
        "outputId": "8b1c6830-d3c3-41a4-d05e-2dc524af3e32"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
            "<ipython-input-49-bcc7fa24eae6>:19: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-4\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'nam...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-bcc7fa24eae6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Initialize the LLM (e.g., GPT-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create a RetrievalQA chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'nam...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
          ]
        }
      ]
    }
  ]
}